# -*- coding: utf-8 -*-
"""DL_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sg6PkqwSudvvwxPtRfgpSqisLF1ZUgl3
"""

from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy

plt.ion()   # interactive mode

seed = 42
np.random.seed(seed)
torch.manual_seed(seed)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

from google.colab import auth
auth.authenticate_user()

from google.colab import drive
drive.mount('/content/gdrive')

datadir = "/content/gdrive/My Drive/Colab Notebooks/bullying_pics"
def load_split_train_test(datadir, valid_size = .3, test_size = 0.33):
  train_transforms = transforms.Compose([
          transforms.RandomRotation(45),
          transforms.RandomResizedCrop(224),
          transforms.RandomHorizontalFlip(),
          transforms.ToTensor(),
          transforms.Normalize([0.485, 0.456, 0.406], 
                             [0.229, 0.224, 0.225])
                                       ])
  
  test_transforms = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], 
                             [0.229, 0.224, 0.225])
                                      ])

  train_data = datasets.ImageFolder(datadir, transform=train_transforms)
  test_data = datasets.ImageFolder(datadir, transform=test_transforms)
  val_data = datasets.ImageFolder(datadir, transform=test_transforms)
  num_train = len(train_data)

  
  indices = list(range(num_train))
  split = int(np.floor(valid_size * num_train))

  split2 = int(np.floor(test_size * split))

  
  from torch.utils.data.sampler import SubsetRandomSampler
  train_idx, test_idx = indices[split:], indices[:split]
  val_idx, test_idx = test_idx[split2:], test_idx[:split2]
  train_sampler = SubsetRandomSampler(train_idx)
  test_sampler = SubsetRandomSampler(test_idx)
  val_sampler = SubsetRandomSampler(val_idx)
  trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=3,num_workers=2)
  testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=3, num_workers=2)
  valloader = torch.utils.data.DataLoader(val_data, sampler=val_sampler, batch_size=3, num_workers=2)
 
  test_dataset_size=len(test_sampler)
  train_dataset_size=len(train_sampler)
  val_dataset_size=len(val_sampler)
  class_names = trainloader.dataset.classes

  return trainloader, testloader ,valloader, class_names , train_dataset_size, test_dataset_size, val_dataset_size, 

trainloader, testloader, valloader, class_names, train_dataset_size, test_dataset_size, val_dataset_size  = load_split_train_test(datadir, .3, 0.33)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

import torch.nn as nn
import torch.nn.functional as F

class SEC_NET(nn.Module):
    _net_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']
    
    def __init__(self, input_channels, class_size=9, weights=True, batch_norm=True):
        super(SEC_NET, self).__init__()
        '''
        self.architecture = nn.Sequential(
            nn.Conv2d(3, 64, 3),
            nn.ReLU(True),
            nn.Conv2d(64, 64, 3),
            nn.ReLU(True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3),
            nn.ReLU(True),
            nn.Conv2d(128, 128, 3),
            nn.ReLU(True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(128, 256, 3),
            nn.ReLU(True),
            nn.Conv2d(256, 256, 3),
            nn.ReLU(True),
            nn.Conv2d(256, 256, 3),
            nn.ReLU(True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(256, 512, 3),
            nn.ReLU(True),
            nn.Conv2d(512, 512, 3),
            nn.ReLU(True),
            nn.Conv2d(512, 512, 3),
            nn.ReLU(True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(512, 512, 3),
            nn.ReLU(True),
            nn.Conv2d(512, 512, 3),
            nn.ReLU(True),
            nn.Conv2d(512, 512, 3),
            nn.ReLU(True),
            nn.MaxPool2d(2, 2),
        )
        '''
        self.batch_norm = batch_norm 
        
        net_layers = []
        
        for config in self._net_config:
            if config != 'M':
                conv = nn.Conv2d(input_channels, config, kernel_size=3, padding=1)
                if self.batch_norm:
                    net_layers += [conv, nn.BatchNorm2d(config), nn.ReLU(True)]
                else:
                    net_layers += [conv, nn.ReLU(True)]
                # set the input of the next convolution to the output of the previous convolution
                input_channels = config
            else:
                net_layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
                
        self.architecture = nn.Sequential(*net_layers)
        self.average_pool = nn.AdaptiveAvgPool2d((7,7))
        self.fully_connected = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, class_size),
        )
        
        if weights:
            self._configure_weights()
    
    #initialize weights using He's technique
    def _configure_weights(self):
        for index, module in enumerate(self.modules()):
            
            if isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm2d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, 0, 0.01)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        x = self.architecture(x)
       
        x = self.average_pool(x)
        x = x.view(x.size(0), -1)  #reshape, let row be the size of x and column inferred from dimension 
        x = self.fully_connected(x)
        return x


    
net = SEC_NET(input_channels=3, class_size=10)
net = net.to(device)

net.load_state_dict(torch.load("/content/gdrive/My Drive/Colab Notebooks/model_train.pt"))

#original- don't change or delete for now till pretrained is working!!!
def train_model(net, learning_rate=0.001, momentum=0.9, weight_decay=0.0005, epoch_size=2):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)

    lr_reduction_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

    for epoch in range(epoch_size):  # loop over the dataset multiple times
        #lr_reduction_scheduler.step()
        training_loss = 0.0
        for i, data in enumerate(trainloader):
            # get the inputs
            train_x, train_y = data
            train_x = train_x.to(device)
            train_y = train_y.to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            output = net(train_x)
            loss = criterion(output, train_y)
            loss.backward()
            optimizer.step()

            # print statistics
            training_loss += loss.item()
            
            if i % 50 == 49:    # print every 50 mini-batches
                
                training_loss = 0.0
                
    return net

model1 = train_model(net)

correct_predictions = 0.0
for i, data in enumerate(valloader):
    test_x, test_y = data
    test_x = test_x.to(device)
    test_y = test_y.to(device)
    
    outputs = net(test_x)
    _, model_prediction = torch.max(outputs.data, 1)
    
torch.save(model1.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/model_train.pt')

from PIL import Image
def predict(image_name):
  imsize = 224
  loader = transforms.Compose([transforms.Scale(imsize), transforms.ToTensor()])

  """load image, returns cuda tensor"""
  image = Image.open(image_name)
  image = loader(image).float()
  #image = Variable(image, requires_grad=True)
  image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet
  return image.cuda()

import sys
filename = sys.argv[-1]

path=sys.path.append(filename)
image = predict(path)
output = net(image)
_, predicted = torch.max(output, 1)

print(class_names[predicted])
